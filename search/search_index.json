{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home Cluster \u00b6 Welcome to the docs on my home Kubernetes cluster. Story \u00b6 In 2018 my homelab setup was very different that what it is today. I was running a Docker Swarm cluster and doing deployments with Portainer . While this was working, I was unsure about Docker Swarm's future and I was also looking at a better way to handle application updates. I knew I wanted to keep using open source software and decided it was a good time to start getting familiar with Kubernetes... in came k3s. k3s \u00b6 k3s comes with a very low barrier to entry in getting a Kubernetes cluster running. After deploying it with k3sup I fell in love with the simplicity and love the single binary approach. With k3sup the time it took from getting a Kubernetes cluster up and running was literally minutes. Without much Kubernetes knowledge this allowed me to teardown the cluster and install it again, each time learning new things like how to deploy applications with Helm , set up a load balancer using Metallb , how to handle ingress and storage. Flux \u00b6 After awhile of tinkering with k3s, I started reading up on GitOps principles and fell in love with the idea of having a Git repository drive a Kubernetes cluster state. No more missing configuration files, backing up compose files in fear of losing them. I could have mostly everything Kubernetes cares about tracked in a Git repo, while having an operator running in my cluster reading from my Git repo. This is where Flux comes into play. Flux is an awesome tool that syncs my Git repo with my cluster. Any change I make in my Git repo will be directly applied by Flux in my Kubernetes cluster. Renovate \u00b6 So I have my cluster running, I have Flux running inside and it is synced to my Git repository. How do I handle application updates? Flux has this built into their application using the image-automation-controller but I am not a fan of having applications auto-update. Sometimes I want to read application changelogs or review source code before I commit to upgrading. I decided that Renovate would be a good solution to my problem. Renovate works by scanning my Git repository and offering changes in the form of a pull request when a new container image update or helm chart update is found. Conclusion \u00b6 Overall I am very happy being off my Portainer/Docker Swarm cluster and finally using Kubernetes. The road was rocky but with passion and perseverance I was able to reach my goal. It should go without saying a lot of what is built upon here is not only my ideas. A lot of work is being done by other people in the k8s@home community.","title":"Introduction"},{"location":"#home-cluster","text":"Welcome to the docs on my home Kubernetes cluster.","title":"Home Cluster"},{"location":"#story","text":"In 2018 my homelab setup was very different that what it is today. I was running a Docker Swarm cluster and doing deployments with Portainer . While this was working, I was unsure about Docker Swarm's future and I was also looking at a better way to handle application updates. I knew I wanted to keep using open source software and decided it was a good time to start getting familiar with Kubernetes... in came k3s.","title":"Story"},{"location":"#k3s","text":"k3s comes with a very low barrier to entry in getting a Kubernetes cluster running. After deploying it with k3sup I fell in love with the simplicity and love the single binary approach. With k3sup the time it took from getting a Kubernetes cluster up and running was literally minutes. Without much Kubernetes knowledge this allowed me to teardown the cluster and install it again, each time learning new things like how to deploy applications with Helm , set up a load balancer using Metallb , how to handle ingress and storage.","title":"k3s"},{"location":"#flux","text":"After awhile of tinkering with k3s, I started reading up on GitOps principles and fell in love with the idea of having a Git repository drive a Kubernetes cluster state. No more missing configuration files, backing up compose files in fear of losing them. I could have mostly everything Kubernetes cares about tracked in a Git repo, while having an operator running in my cluster reading from my Git repo. This is where Flux comes into play. Flux is an awesome tool that syncs my Git repo with my cluster. Any change I make in my Git repo will be directly applied by Flux in my Kubernetes cluster.","title":"Flux"},{"location":"#renovate","text":"So I have my cluster running, I have Flux running inside and it is synced to my Git repository. How do I handle application updates? Flux has this built into their application using the image-automation-controller but I am not a fan of having applications auto-update. Sometimes I want to read application changelogs or review source code before I commit to upgrading. I decided that Renovate would be a good solution to my problem. Renovate works by scanning my Git repository and offering changes in the form of a pull request when a new container image update or helm chart update is found.","title":"Renovate"},{"location":"#conclusion","text":"Overall I am very happy being off my Portainer/Docker Swarm cluster and finally using Kubernetes. The road was rocky but with passion and perseverance I was able to reach my goal. It should go without saying a lot of what is built upon here is not only my ideas. A lot of work is being done by other people in the k8s@home community.","title":"Conclusion"},{"location":"calico-metrics/","text":"calico metrics \u00b6 calico-node \u00b6 calicoctl patch felixConfiguration default --patch '{\"spec\":{\"prometheusMetricsEnabled\": true}}' kubectl -n calico-system edit ds calico-node Under spec.template.spec.containers : # ... ports : - containerPort : 9091 name : http-metrics protocol : TCP # ... calico-typha \u00b6 kubectl -n calico-system edit deployment calico-typha Under spec.template.spec.containers : # ... - env : - name : TYPHA_PROMETHEUSMETRICSENABLED value : \"true\" - name : TYPHA_PROMETHEUSMETRICSPORT value : \"9092\" # ... ports : - containerPort : 9092 name : http-metrics protocol : TCP # ... calico-kube-controllers \u00b6 This is not working I am unable to patch kubecontrollersconfiguration with the prometheus port calicoctl patch kubecontrollersconfiguration default --patch '{\"spec\":{\"prometheusMetricsPort\": 9094}}' kubectl -n calico-system edit deployment calico-kube-controllers Under spec.template.spec.containers : # ... ports : - containerPort : 9094 name : http-metrics protocol : TCP # ...","title":"calico metrics"},{"location":"calico-metrics/#calico-metrics","text":"","title":"calico metrics"},{"location":"calico-metrics/#calico-node","text":"calicoctl patch felixConfiguration default --patch '{\"spec\":{\"prometheusMetricsEnabled\": true}}' kubectl -n calico-system edit ds calico-node Under spec.template.spec.containers : # ... ports : - containerPort : 9091 name : http-metrics protocol : TCP # ...","title":"calico-node"},{"location":"calico-metrics/#calico-typha","text":"kubectl -n calico-system edit deployment calico-typha Under spec.template.spec.containers : # ... - env : - name : TYPHA_PROMETHEUSMETRICSENABLED value : \"true\" - name : TYPHA_PROMETHEUSMETRICSPORT value : \"9092\" # ... ports : - containerPort : 9092 name : http-metrics protocol : TCP # ...","title":"calico-typha"},{"location":"calico-metrics/#calico-kube-controllers","text":"This is not working I am unable to patch kubecontrollersconfiguration with the prometheus port calicoctl patch kubecontrollersconfiguration default --patch '{\"spec\":{\"prometheusMetricsPort\": 9094}}' kubectl -n calico-system edit deployment calico-kube-controllers Under spec.template.spec.containers : # ... ports : - containerPort : 9094 name : http-metrics protocol : TCP # ...","title":"calico-kube-controllers"},{"location":"external-secrets/","text":"External Secrets \u00b6 Work in progress This document is a work in progress. Create secret for External Secrets using AWS Secrets Manager \u00b6 kubectl create secret generic aws-credentials \\ --from-literal = id = \"access-key-id\" \\ --from-literal = key = \"access-secret-key\" \\ --namespace kube-system Create a secret using aws-cli \u00b6 aws secretsmanager create-secret \\ --name namespace/secret-name \\ --secret-string \"secret-data\"","title":"External Secrets"},{"location":"external-secrets/#external-secrets","text":"Work in progress This document is a work in progress.","title":"External Secrets"},{"location":"external-secrets/#create-secret-for-external-secrets-using-aws-secrets-manager","text":"kubectl create secret generic aws-credentials \\ --from-literal = id = \"access-key-id\" \\ --from-literal = key = \"access-secret-key\" \\ --namespace kube-system","title":"Create secret for External Secrets using AWS Secrets Manager"},{"location":"external-secrets/#create-a-secret-using-aws-cli","text":"aws secretsmanager create-secret \\ --name namespace/secret-name \\ --secret-string \"secret-data\"","title":"Create a secret using aws-cli"},{"location":"flux/","text":"Flux \u00b6 Work in progress This document is a work in progress. Install the CLI tool \u00b6 brew install fluxcd/tap/flux Install the cluster components \u00b6 For full installation guide visit the Flux installation guide Check if you cluster is ready for Flux flux check --pre Install Flux into your cluster set -x GITHUB_TOKEN xyz ; flux bootstrap github \\ --version = v0.12.1 \\ --owner = onedr0p \\ --repository = home-cluster \\ --path = cluster/base \\ --personal \\ --private = false \\ --network-policy = false Note : When using k3s I found that the network-policy flag has to be set to false, or Flux will not work Useful commands \u00b6 Force flux to sync your repository: flux reconcile source git flux-system Force flux to sync a helm release: flux reconcile helmrelease sonarr -n default Force flux to sync a helm repository: flux reconcile source helm ingress-nginx-charts -n flux-system","title":"Flux"},{"location":"flux/#flux","text":"Work in progress This document is a work in progress.","title":"Flux"},{"location":"flux/#install-the-cli-tool","text":"brew install fluxcd/tap/flux","title":"Install the CLI tool"},{"location":"flux/#install-the-cluster-components","text":"For full installation guide visit the Flux installation guide Check if you cluster is ready for Flux flux check --pre Install Flux into your cluster set -x GITHUB_TOKEN xyz ; flux bootstrap github \\ --version = v0.12.1 \\ --owner = onedr0p \\ --repository = home-cluster \\ --path = cluster/base \\ --personal \\ --private = false \\ --network-policy = false Note : When using k3s I found that the network-policy flag has to be set to false, or Flux will not work","title":"Install the cluster components"},{"location":"flux/#useful-commands","text":"Force flux to sync your repository: flux reconcile source git flux-system Force flux to sync a helm release: flux reconcile helmrelease sonarr -n default Force flux to sync a helm repository: flux reconcile source helm ingress-nginx-charts -n flux-system","title":"Useful commands"},{"location":"rook-ceph-maintenance/","text":"Rook-Ceph Maintenance \u00b6 Work in progress This document is a work in progress. Accessing volumes \u00b6 Sometimes I am required to access the data in the pvc , below is an example on how I access the pvc data for my zigbee2mqtt deployment. First start by scaling the app deployment to 0 replicas: kubectl scale deploy/zigbee2mqtt --replicas 0 -n home Get the rbd image name for the app: kubectl get pv/ ( kubectl get pv | grep plex-config-v1 | awk -F ' ' '{print $1}' ) -n home -o json | jq -r '.spec.csi.volumeAttributes.imageName' Exec into the rook-direct-mount toolbox: kubectl -n rook-ceph exec -it ( kubectl -n rook-ceph get pod -l \"app=rook-direct-mount\" -o jsonpath = '{.items[0].metadata.name}' ) bash Create a directory to mount the volume to: mkdir -p /mnt/data Mounting a NFS share This can be useful if you want to move data from or to a nfs share Create a directory to mount the nfs share to: mkdir -p /mnt/nfsdata Mount the nfs share: mount -t nfs -o \"tcp,intr,rw,noatime,nodiratime,rsize=65536,wsize=65536,hard\" 192 .168.42.50:/volume1/Data /mnt/nfs List all the rbd block device names: rbd list --pool replicapool Map the rbd block device to a /dev/rbdX device: rbd map -p replicapool csi-vol-9a010830-8b0a-11eb-b291-6aaa17155076 Mount the /dev/rbdX device: mount /dev/rbdX /mnt/data At this point you'll be able to access the volume data under /mnt/data , you can change files in any way. Backing up or restoring data from a NFS share Restoring data: rm -rf /mnt/data/* tar xvf /mnt/nfsdata/backups/zigbee2mqtt.tar.gz -C /mnt/data chown -R 568 :568 /mnt/data/ Backing up data: tar czvf /mnt/nfsdata/backups/zigbee2mqtt.tar.gz -C /mnt/data/ . When done you can unmount /mnt/data and unmap the rbd device: umount /mnt/data rbd unmap -p replicapool csi-vol-9a010830-8b0a-11eb-b291-6aaa17155076 Lastly you need to scale the deployment replicas back up to 1: kubectl scale deploy/zigbee2mqtt --replicas 1 -n home Handling crashes \u00b6 Sometimes rook-ceph will report a HEALTH_WARN even when the health is fine, in order to get ceph to report back healthy do the following... # list all the crashes ceph crash ls # if you want to read the message ceph crash info <id> # archive crash report ceph crash archive <id> # or, archive all crash reports ceph crash archive-all Helpful links \u00b6 Common issues kubectl -n rook-ceph exec -it (kubectl -n rook-ceph get pod -l \"app=rook-direct-mount\" -o jsonpath='{.items[0].metadata.name}') bash mount -t nfs -o \"tcp,intr,rw,noatime,nodiratime,rsize=65536,wsize=65536,hard\" 192.168.42.50:/volume1/Data /mnt/nfs kubectl get pv/(kubectl get pv \\ | grep \"tautulli\" \\ | awk -F' ' '{print $1}') -n media -o json \\ | jq -r '.spec.csi.volumeAttributes.imageName' rbd map -p replicapool csi-vol-2bd198a6-9a7d-11eb-ae97-9a71104156fa \\ | xargs -I{} mount {} /mnt/data rm -rf /mnt/data/* tar xvf /mnt/nfs/backups/tautulli.tar.gz -C /mnt/data # chown -R 568:568 /mnt/data/ umount /mnt/data && \\ rbd unmap -p replicapool csi-vol-2bd198a6-9a7d-11eb-ae97-9a71104156fa ceph mgr module enable rook ceph orch set backend rook ceph dashboard set-alertmanager-api-host http://kube-prometheus-stack-alertmanager.monitoring.svc:9093 ceph dashboard set-prometheus-api-host http://kube-prometheus-stack-prometheus.monitoring.svc:9090","title":"Rook-Ceph Maintenance"},{"location":"rook-ceph-maintenance/#rook-ceph-maintenance","text":"Work in progress This document is a work in progress.","title":"Rook-Ceph Maintenance"},{"location":"rook-ceph-maintenance/#accessing-volumes","text":"Sometimes I am required to access the data in the pvc , below is an example on how I access the pvc data for my zigbee2mqtt deployment. First start by scaling the app deployment to 0 replicas: kubectl scale deploy/zigbee2mqtt --replicas 0 -n home Get the rbd image name for the app: kubectl get pv/ ( kubectl get pv | grep plex-config-v1 | awk -F ' ' '{print $1}' ) -n home -o json | jq -r '.spec.csi.volumeAttributes.imageName' Exec into the rook-direct-mount toolbox: kubectl -n rook-ceph exec -it ( kubectl -n rook-ceph get pod -l \"app=rook-direct-mount\" -o jsonpath = '{.items[0].metadata.name}' ) bash Create a directory to mount the volume to: mkdir -p /mnt/data Mounting a NFS share This can be useful if you want to move data from or to a nfs share Create a directory to mount the nfs share to: mkdir -p /mnt/nfsdata Mount the nfs share: mount -t nfs -o \"tcp,intr,rw,noatime,nodiratime,rsize=65536,wsize=65536,hard\" 192 .168.42.50:/volume1/Data /mnt/nfs List all the rbd block device names: rbd list --pool replicapool Map the rbd block device to a /dev/rbdX device: rbd map -p replicapool csi-vol-9a010830-8b0a-11eb-b291-6aaa17155076 Mount the /dev/rbdX device: mount /dev/rbdX /mnt/data At this point you'll be able to access the volume data under /mnt/data , you can change files in any way. Backing up or restoring data from a NFS share Restoring data: rm -rf /mnt/data/* tar xvf /mnt/nfsdata/backups/zigbee2mqtt.tar.gz -C /mnt/data chown -R 568 :568 /mnt/data/ Backing up data: tar czvf /mnt/nfsdata/backups/zigbee2mqtt.tar.gz -C /mnt/data/ . When done you can unmount /mnt/data and unmap the rbd device: umount /mnt/data rbd unmap -p replicapool csi-vol-9a010830-8b0a-11eb-b291-6aaa17155076 Lastly you need to scale the deployment replicas back up to 1: kubectl scale deploy/zigbee2mqtt --replicas 1 -n home","title":"Accessing volumes"},{"location":"rook-ceph-maintenance/#handling-crashes","text":"Sometimes rook-ceph will report a HEALTH_WARN even when the health is fine, in order to get ceph to report back healthy do the following... # list all the crashes ceph crash ls # if you want to read the message ceph crash info <id> # archive crash report ceph crash archive <id> # or, archive all crash reports ceph crash archive-all","title":"Handling crashes"},{"location":"rook-ceph-maintenance/#helpful-links","text":"Common issues kubectl -n rook-ceph exec -it (kubectl -n rook-ceph get pod -l \"app=rook-direct-mount\" -o jsonpath='{.items[0].metadata.name}') bash mount -t nfs -o \"tcp,intr,rw,noatime,nodiratime,rsize=65536,wsize=65536,hard\" 192.168.42.50:/volume1/Data /mnt/nfs kubectl get pv/(kubectl get pv \\ | grep \"tautulli\" \\ | awk -F' ' '{print $1}') -n media -o json \\ | jq -r '.spec.csi.volumeAttributes.imageName' rbd map -p replicapool csi-vol-2bd198a6-9a7d-11eb-ae97-9a71104156fa \\ | xargs -I{} mount {} /mnt/data rm -rf /mnt/data/* tar xvf /mnt/nfs/backups/tautulli.tar.gz -C /mnt/data # chown -R 568:568 /mnt/data/ umount /mnt/data && \\ rbd unmap -p replicapool csi-vol-2bd198a6-9a7d-11eb-ae97-9a71104156fa ceph mgr module enable rook ceph orch set backend rook ceph dashboard set-alertmanager-api-host http://kube-prometheus-stack-alertmanager.monitoring.svc:9093 ceph dashboard set-prometheus-api-host http://kube-prometheus-stack-prometheus.monitoring.svc:9090","title":"Helpful links"},{"location":"rook-ceph-vol-migration-draft/","text":"k scale deployment/frigate -n home --replicas 0 kubectl get pv/(kubectl get pv | grep \"frigate-config[[:space:]+]\" | awk -F' ' '{print $1}') -n home -o json | jq -r '.spec.csi.volumeAttributes.imageName' csi-vol-e210e08c-80f5-11eb-bb77-f25ddf8c8685 \u00b6 kubectl get pv/(kubectl get pv | grep \"frigate-config-v1[[:space:]+]\" | awk -F' ' '{print $1}') -n home -o json | jq -r '.spec.csi.volumeAttributes.imageName' csi-vol-de945d8c-8fc2-11eb-b291-6aaa17155076 \u00b6 Toolbox \u00b6 Access rook direct mount kubectl -n rook-ceph exec -it ( kubectl -n rook-ceph get pod -l \"app=rook-direct-mount\" -o jsonpath = '{.items[0].metadata.name}' ) bash mkdir -p /mnt/ { old,new } rbd map -p replicapool csi-vol-e210e08c-80f5-11eb-bb77-f25ddf8c8685 | xargs -I {} mount {} /mnt/old rbd map -p replicapool csi-vol-de945d8c-8fc2-11eb-b291-6aaa17155076 | xargs -0 -I {} sh -c 'mkfs.ext4 {}; mount {} /mnt/new' cp -rp /mnt/old/. /mnt/new chown -R 568 :568 /mnt/new umount /mnt/old umount /mnt/new rbd unmap -p replicapool csi-vol-e210e08c-80f5-11eb-bb77-f25ddf8c8685 && \\ rbd unmap -p replicapool csi-vol-de945d8c-8fc2-11eb-b291-6aaa17155076","title":"Rook ceph vol migration draft"},{"location":"rook-ceph-vol-migration-draft/#csi-vol-e210e08c-80f5-11eb-bb77-f25ddf8c8685","text":"kubectl get pv/(kubectl get pv | grep \"frigate-config-v1[[:space:]+]\" | awk -F' ' '{print $1}') -n home -o json | jq -r '.spec.csi.volumeAttributes.imageName'","title":"csi-vol-e210e08c-80f5-11eb-bb77-f25ddf8c8685"},{"location":"rook-ceph-vol-migration-draft/#csi-vol-de945d8c-8fc2-11eb-b291-6aaa17155076","text":"","title":"csi-vol-de945d8c-8fc2-11eb-b291-6aaa17155076"},{"location":"rook-ceph-vol-migration-draft/#toolbox","text":"Access rook direct mount kubectl -n rook-ceph exec -it ( kubectl -n rook-ceph get pod -l \"app=rook-direct-mount\" -o jsonpath = '{.items[0].metadata.name}' ) bash mkdir -p /mnt/ { old,new } rbd map -p replicapool csi-vol-e210e08c-80f5-11eb-bb77-f25ddf8c8685 | xargs -I {} mount {} /mnt/old rbd map -p replicapool csi-vol-de945d8c-8fc2-11eb-b291-6aaa17155076 | xargs -0 -I {} sh -c 'mkfs.ext4 {}; mount {} /mnt/new' cp -rp /mnt/old/. /mnt/new chown -R 568 :568 /mnt/new umount /mnt/old umount /mnt/new rbd unmap -p replicapool csi-vol-e210e08c-80f5-11eb-bb77-f25ddf8c8685 && \\ rbd unmap -p replicapool csi-vol-de945d8c-8fc2-11eb-b291-6aaa17155076","title":"Toolbox"},{"location":"sealed-secrets/","text":"Sealed Secrets \u00b6 Work in progress This document is a work in progress. Install the CLI tool \u00b6 brew install kubeseal Install the cluster components \u00b6 --- apiVersion : helm.toolkit.fluxcd.io/v2beta1 kind : HelmRelease metadata : name : sealed-secrets namespace : kube-system spec : interval : 5m chart : spec : chart : sealed-secrets version : 1.13.2 sourceRef : kind : HelmRepository name : sealed-secrets-charts namespace : flux-system interval : 5m values : ingress : enabled : false Fetch the Sealed Secrets public certificate \u00b6 kubeseal \\ --controller-name sealed-secrets \\ --fetch-cert > ./sealed-secrets-public-cert.pem","title":"Sealed Secrets"},{"location":"sealed-secrets/#sealed-secrets","text":"Work in progress This document is a work in progress.","title":"Sealed Secrets"},{"location":"sealed-secrets/#install-the-cli-tool","text":"brew install kubeseal","title":"Install the CLI tool"},{"location":"sealed-secrets/#install-the-cluster-components","text":"--- apiVersion : helm.toolkit.fluxcd.io/v2beta1 kind : HelmRelease metadata : name : sealed-secrets namespace : kube-system spec : interval : 5m chart : spec : chart : sealed-secrets version : 1.13.2 sourceRef : kind : HelmRepository name : sealed-secrets-charts namespace : flux-system interval : 5m values : ingress : enabled : false","title":"Install the cluster components"},{"location":"sealed-secrets/#fetch-the-sealed-secrets-public-certificate","text":"kubeseal \\ --controller-name sealed-secrets \\ --fetch-cert > ./sealed-secrets-public-cert.pem","title":"Fetch the Sealed Secrets public certificate"},{"location":"snmp-exporter/","text":"SNMP Exporter \u00b6 Work in progress This document is a work in progress. I am using snmp-exporter for getting metrics from my Cyberpower PDUs ( PDU41001 ) and my APC UPS ( Smart-UPS 1500 ) into Prometheus Clone and build the snmp-exporter generator \u00b6 sudo apt-get install unzip build-essential libsnmp-dev golang go get github.com/prometheus/snmp_exporter/generator cd ${ GOPATH - $HOME /go } /src/github.com/prometheus/snmp_exporter/generator go build make mibs Update generator.yml \u00b6 Dealing with configmaps Kubernetes configmap 's have a max size. I needed to strip out all the other modules. modules : apcups : version : 1 walk : - sysUpTime - interfaces - 1.3.6.1.4.1.318.1.1.1.2 # upsBattery - 1.3.6.1.4.1.318.1.1.1.3 # upsInput - 1.3.6.1.4.1.318.1.1.1.4 # upsOutput - 1.3.6.1.4.1.318.1.1.1.7.2 # upsAdvTest - 1.3.6.1.4.1.318.1.1.1.8.1 # upsCommStatus - 1.3.6.1.4.1.318.1.1.1.12 # upsOutletGroups - 1.3.6.1.4.1.318.1.1.10.2.3.2 # iemStatusProbesTable - 1.3.6.1.4.1.318.1.1.26.8.3 # rPDU2BankStatusTable lookups : - source_indexes : [ upsOutletGroupStatusIndex ] lookup : upsOutletGroupStatusName drop_source_indexes : true - source_indexes : [ iemStatusProbeIndex ] lookup : iemStatusProbeName drop_source_indexes : true overrides : ifType : type : EnumAsInfo rPDU2BankStatusLoadState : type : EnumAsStateSet upsAdvBatteryCondition : type : EnumAsStateSet upsAdvBatteryChargingCurrentRestricted : type : EnumAsStateSet upsAdvBatteryChargerStatus : type : EnumAsStateSet cyberpower : version : 1 walk : - ePDUIdentName - ePDUIdentHardwareRev - ePDUStatusInputVoltage ## input voltage (0.1 volts) - ePDUStatusInputFrequency ## input frequency (0.1 Hertz) - ePDULoadStatusLoad ## load (tenths of Amps) - ePDULoadStatusVoltage ## voltage (0.1 volts) - ePDULoadStatusActivePower ## active power (watts) - ePDULoadStatusApparentPower ## apparent power (VA) - ePDULoadStatusPowerFactor ## power factor of the output (hundredths) - ePDULoadStatusEnergy ## apparent power measured (0.1 kw/h). - ePDUOutletControlOutletName ## The name of the outlet. - ePDUOutletStatusLoad ## Outlet load (tenths of Amps) - ePDUOutletStatusActivePower ## Outlet load (watts) - envirTemperature ## temp expressed (1/10 \u00baF) - envirTemperatureCelsius ## temp expressed (1/10 \u00baF) - envirHumidity ## relative humidity (%) Get the Cyberpower MIB \u00b6 wget https://dl4jz3rbrsfum.cloudfront.net/software/CyberPower_MIB_v2.9.MIB.zip unzip CyberPower_MIB_v2.9.MIB.zip mv CyberPower_MIB_v2.9.MIB mibs/ Generate the snmp.yml \u00b6 This will create a snmp.yml file which will be needed for the configmap for the snmp-exporter deployment export MIBDIRS = mibs ./generator generate","title":"SNMP Exporter"},{"location":"snmp-exporter/#snmp-exporter","text":"Work in progress This document is a work in progress. I am using snmp-exporter for getting metrics from my Cyberpower PDUs ( PDU41001 ) and my APC UPS ( Smart-UPS 1500 ) into Prometheus","title":"SNMP Exporter"},{"location":"snmp-exporter/#clone-and-build-the-snmp-exporter-generator","text":"sudo apt-get install unzip build-essential libsnmp-dev golang go get github.com/prometheus/snmp_exporter/generator cd ${ GOPATH - $HOME /go } /src/github.com/prometheus/snmp_exporter/generator go build make mibs","title":"Clone and build the snmp-exporter generator"},{"location":"snmp-exporter/#update-generatoryml","text":"Dealing with configmaps Kubernetes configmap 's have a max size. I needed to strip out all the other modules. modules : apcups : version : 1 walk : - sysUpTime - interfaces - 1.3.6.1.4.1.318.1.1.1.2 # upsBattery - 1.3.6.1.4.1.318.1.1.1.3 # upsInput - 1.3.6.1.4.1.318.1.1.1.4 # upsOutput - 1.3.6.1.4.1.318.1.1.1.7.2 # upsAdvTest - 1.3.6.1.4.1.318.1.1.1.8.1 # upsCommStatus - 1.3.6.1.4.1.318.1.1.1.12 # upsOutletGroups - 1.3.6.1.4.1.318.1.1.10.2.3.2 # iemStatusProbesTable - 1.3.6.1.4.1.318.1.1.26.8.3 # rPDU2BankStatusTable lookups : - source_indexes : [ upsOutletGroupStatusIndex ] lookup : upsOutletGroupStatusName drop_source_indexes : true - source_indexes : [ iemStatusProbeIndex ] lookup : iemStatusProbeName drop_source_indexes : true overrides : ifType : type : EnumAsInfo rPDU2BankStatusLoadState : type : EnumAsStateSet upsAdvBatteryCondition : type : EnumAsStateSet upsAdvBatteryChargingCurrentRestricted : type : EnumAsStateSet upsAdvBatteryChargerStatus : type : EnumAsStateSet cyberpower : version : 1 walk : - ePDUIdentName - ePDUIdentHardwareRev - ePDUStatusInputVoltage ## input voltage (0.1 volts) - ePDUStatusInputFrequency ## input frequency (0.1 Hertz) - ePDULoadStatusLoad ## load (tenths of Amps) - ePDULoadStatusVoltage ## voltage (0.1 volts) - ePDULoadStatusActivePower ## active power (watts) - ePDULoadStatusApparentPower ## apparent power (VA) - ePDULoadStatusPowerFactor ## power factor of the output (hundredths) - ePDULoadStatusEnergy ## apparent power measured (0.1 kw/h). - ePDUOutletControlOutletName ## The name of the outlet. - ePDUOutletStatusLoad ## Outlet load (tenths of Amps) - ePDUOutletStatusActivePower ## Outlet load (watts) - envirTemperature ## temp expressed (1/10 \u00baF) - envirTemperatureCelsius ## temp expressed (1/10 \u00baF) - envirHumidity ## relative humidity (%)","title":"Update generator.yml"},{"location":"snmp-exporter/#get-the-cyberpower-mib","text":"wget https://dl4jz3rbrsfum.cloudfront.net/software/CyberPower_MIB_v2.9.MIB.zip unzip CyberPower_MIB_v2.9.MIB.zip mv CyberPower_MIB_v2.9.MIB mibs/","title":"Get the Cyberpower MIB"},{"location":"snmp-exporter/#generate-the-snmpyml","text":"This will create a snmp.yml file which will be needed for the configmap for the snmp-exporter deployment export MIBDIRS = mibs ./generator generate","title":"Generate the snmp.yml"},{"location":"velero/","text":"Velero \u00b6 Work in progress This document is a work in progress. Install the CLI tool \u00b6 brew install velero Create a backup \u00b6 Create a backup for all apps: velero backup create manually-backup-1 --from-schedule velero-daily-backup Create a backup for a single app: velero backup create jackett-test-abc \\ --include-namespaces testing \\ --selector \"app.kubernetes.io/instance=jackett-test\" \\ --wait Delete resources \u00b6 Delete the HelmRelease : kubectl delete hr jackett-test -n testing Wait Allow the application to be redeployed and create the new resources Delete the new resources: kubectl delete deployment/jackett-test -n jackett kubectl delete pvc/jackett-test-config Restore \u00b6 velero restore create \\ --from-backup velero-daily-backup-20201120020022 \\ --include-namespaces testing \\ --selector \"app.kubernetes.io/instance=jackett-test\" \\ --wait","title":"Velero"},{"location":"velero/#velero","text":"Work in progress This document is a work in progress.","title":"Velero"},{"location":"velero/#install-the-cli-tool","text":"brew install velero","title":"Install the CLI tool"},{"location":"velero/#create-a-backup","text":"Create a backup for all apps: velero backup create manually-backup-1 --from-schedule velero-daily-backup Create a backup for a single app: velero backup create jackett-test-abc \\ --include-namespaces testing \\ --selector \"app.kubernetes.io/instance=jackett-test\" \\ --wait","title":"Create a backup"},{"location":"velero/#delete-resources","text":"Delete the HelmRelease : kubectl delete hr jackett-test -n testing Wait Allow the application to be redeployed and create the new resources Delete the new resources: kubectl delete deployment/jackett-test -n jackett kubectl delete pvc/jackett-test-config","title":"Delete resources"},{"location":"velero/#restore","text":"velero restore create \\ --from-backup velero-daily-backup-20201120020022 \\ --include-namespaces testing \\ --selector \"app.kubernetes.io/instance=jackett-test\" \\ --wait","title":"Restore"},{"location":"opnsense/bgp/","text":"Opnsense | BGP \u00b6 Work in progress This document is a work in progress.","title":"Opnsense | BGP"},{"location":"opnsense/bgp/#opnsense-bgp","text":"Work in progress This document is a work in progress.","title":"Opnsense | BGP"},{"location":"opnsense/pxe/","text":"Opnsense | PXE \u00b6 Work in progress This document is a work in progress. Setting up TFTP \u00b6 Enable dnsmasq in the Opnsense services settings (set port to 63 ) Copy over pxe.conf to /usr/local/etc/dnsmasq.conf.d/pxe.conf SSH into opnsense and run the following commands... $ mkdir -p /var/lib/tftpboot/pxelinux/ $ wget https://releases.ubuntu.com/20.04/ubuntu-20.04.2-live-server-amd64.iso -O /var/lib/tftpboot/ubuntu-20.04.2-live-server-amd64.iso $ mount -t cd9660 /dev/ ` mdconfig -f /var/lib/tftpboot/ubuntu-20.04.2-live-server-amd64.iso ` /mnt $ cp /mnt/casper/vmlinuz /var/lib/tftpboot/pxelinux/ $ cp /mnt/casper/initrd /var/lib/tftpboot/pxelinux/ $ umount /mnt $ wget http://archive.ubuntu.com/ubuntu/dists/focal/main/uefi/grub2-amd64/current/grubnetx64.efi.signed -O /var/lib/tftpboot/pxelinux/pxelinux.0 Copy grub/grub.conf into /var/lib/tftpboot/grub/grub.conf Copy nodes/ into /var/lib/tftpboot/nodes","title":"Opnsense | PXE"},{"location":"opnsense/pxe/#opnsense-pxe","text":"Work in progress This document is a work in progress.","title":"Opnsense | PXE"},{"location":"opnsense/pxe/#setting-up-tftp","text":"Enable dnsmasq in the Opnsense services settings (set port to 63 ) Copy over pxe.conf to /usr/local/etc/dnsmasq.conf.d/pxe.conf SSH into opnsense and run the following commands... $ mkdir -p /var/lib/tftpboot/pxelinux/ $ wget https://releases.ubuntu.com/20.04/ubuntu-20.04.2-live-server-amd64.iso -O /var/lib/tftpboot/ubuntu-20.04.2-live-server-amd64.iso $ mount -t cd9660 /dev/ ` mdconfig -f /var/lib/tftpboot/ubuntu-20.04.2-live-server-amd64.iso ` /mnt $ cp /mnt/casper/vmlinuz /var/lib/tftpboot/pxelinux/ $ cp /mnt/casper/initrd /var/lib/tftpboot/pxelinux/ $ umount /mnt $ wget http://archive.ubuntu.com/ubuntu/dists/focal/main/uefi/grub2-amd64/current/grubnetx64.efi.signed -O /var/lib/tftpboot/pxelinux/pxelinux.0 Copy grub/grub.conf into /var/lib/tftpboot/grub/grub.conf Copy nodes/ into /var/lib/tftpboot/nodes","title":"Setting up TFTP"}]}